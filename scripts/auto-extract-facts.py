#!/usr/bin/env python3
"""
auto-extract-facts.py - ä» session æ‘˜è¦è‡ªåŠ¨æå–åŸå­äº‹å®å†™å…¥ facts.json

æ ¸å¿ƒæµç¨‹:
  1. æ‰«ææœ¬å‘¨ sessions, æŒ‰é¡¹ç›®åˆ†ç»„
  2. è¿‡æ»¤ >= 2 ä¸ª session çš„é¡¹ç›®
  3. å¯¹æ¯ä¸ªé¡¹ç›®: åŠ è½½å·²æœ‰ facts â†’ ç»„è£… prompt â†’ è°ƒç”¨ haiku â†’ è§£æ â†’ å†™å…¥

é…ç½®:
  é¡¹ç›®æ˜ å°„å’Œå¿½ç•¥åˆ—è¡¨ä» ~/.claude/memory/config.json è¯»å–:
  {
    "project_name_map": {"repo-dir-name": "memory-project-name"},
    "ignored_projects": ["mac", "git-repo"]
  }

ç”¨æ³•:
    python3 auto-extract-facts.py                     # é»˜è®¤æå–æœ¬å‘¨
    python3 auto-extract-facts.py --dry-run            # é¢„è§ˆæ¨¡å¼
    python3 auto-extract-facts.py --project my-api --dry-run
    python3 auto-extract-facts.py --days 14            # æ‰«ææœ€è¿‘ 14 å¤©
"""

import argparse
import json
import os
import re
import subprocess
import sys
from datetime import datetime, timedelta
from pathlib import Path

MEMORY_DIR = Path.home() / ".claude" / "memory"
SESSIONS_DIR = MEMORY_DIR / "sessions"
AREAS_DIR = MEMORY_DIR / "areas" / "projects"
CONFIG_PATH = MEMORY_DIR / "config.json"

# ä»é…ç½®æ–‡ä»¶åŠ è½½é¡¹ç›®æ˜ å°„ï¼ˆæ”¯æŒç”¨æˆ·è‡ªå®šä¹‰ï¼‰
def load_config() -> dict:
    if CONFIG_PATH.is_file():
        try:
            return json.loads(CONFIG_PATH.read_text(encoding="utf-8"))
        except json.JSONDecodeError:
            return {}
    return {}

config = load_config()

# session ç›®å½•å â†’ areas/projects ç›®å½•å
PROJECT_NAME_MAP = config.get("project_name_map", {})

# å¿½ç•¥çš„"é¡¹ç›®"å (éä»£ç é¡¹ç›®)
IGNORED_PROJECTS = set(config.get("ignored_projects", ["mac", "git-repo"]))

VALID_CATEGORIES = [
    "api",
    "architecture",
    "implementation",
    "configuration",
    "bug-pattern",
    "business-logic",
]

LLM_TIMEOUT = 180  # seconds (claude CLI startup + model response)


def resolve_project_name(raw_name: str) -> str | None:
    """å°† session ä¸­çš„é¡¹ç›®åæ˜ å°„åˆ° areas/projects ç›®å½•å"""
    raw_name = raw_name.strip()

    if raw_name in IGNORED_PROJECTS:
        return None

    # å…ˆæŸ¥æ˜ å°„è¡¨
    if raw_name in PROJECT_NAME_MAP:
        return PROJECT_NAME_MAP[raw_name]

    # æ£€æŸ¥ areas/projects/{name}/ æ˜¯å¦å­˜åœ¨
    if (AREAS_DIR / raw_name).is_dir():
        return raw_name

    return None


def scan_sessions(days: int = 7) -> dict[str, list[Path]]:
    """
    æ‰«ææœ€è¿‘ N å¤©çš„ session æ‘˜è¦, æŒ‰é¡¹ç›®åˆ†ç»„.
    åªåŒ…å«ç»“æ„åŒ–æ‘˜è¦ (å« 'å…³é”®çŸ¥è¯†ç‚¹' æˆ– '**ä¸»é¢˜**').

    Returns: {project_name: [session_path, ...]}
    """
    cutoff = datetime.now() - timedelta(days=days)
    grouped: dict[str, list[Path]] = {}

    if not SESSIONS_DIR.is_dir():
        return grouped

    for session_file in sorted(SESSIONS_DIR.glob("*.md")):
        # æŒ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´è¿‡æ»¤
        mtime = datetime.fromtimestamp(session_file.stat().st_mtime)
        if mtime < cutoff:
            continue

        content = session_file.read_text(encoding="utf-8", errors="ignore")

        # åªå¤„ç†ç»“æ„åŒ–æ‘˜è¦
        if "å…³é”®çŸ¥è¯†ç‚¹" not in content and "**ä¸»é¢˜**" not in content:
            continue

        # æå–é¡¹ç›®å
        match = re.search(r"^\- \*\*é¡¹ç›®\*\*:\s*(.+)$", content, re.MULTILINE)
        if not match:
            continue

        raw_project = match.group(1).strip()
        project = resolve_project_name(raw_project)
        if not project:
            continue

        grouped.setdefault(project, []).append(session_file)

    return grouped


def extract_knowledge_points(session_path: Path) -> str:
    """ä» session æ‘˜è¦ä¸­æå–å…³é”®çŸ¥è¯†ç‚¹éƒ¨åˆ†"""
    content = session_path.read_text(encoding="utf-8", errors="ignore")
    lines = content.split("\n")
    result_lines = []
    capturing = False

    for line in lines:
        # æ£€æµ‹çŸ¥è¯†ç‚¹åŒºå—å¼€å§‹ (å¿…é¡»æ˜¯æ ‡é¢˜è¡Œæˆ–åŠ ç²—æ ‡ç­¾ï¼Œå¿½ç•¥æ•£æ–‡ä¸­çš„æåŠ)
        if "å…³é”®çŸ¥è¯†ç‚¹" in line and (
            line.lstrip().startswith("#")
            or line.lstrip().startswith("**")
            or line.lstrip().startswith("- **")
        ):
            capturing = True
            result_lines = [line]  # é‡ç½®ï¼Œå–æœ€ååŒ¹é…çš„åŒºå—
            continue

        if capturing:
            # é‡åˆ°ä¸‹ä¸€ä¸ªåŒçº§æˆ–æ›´é«˜çº§æ ‡é¢˜æ—¶åœæ­¢
            if re.match(r"^#{1,3}\s", line) and "å…³é”®çŸ¥è¯†ç‚¹" not in line:
                break
            # é‡åˆ°å…¶ä»–ä¸»è¦åŒºå—æ ‡è®°ä¹Ÿåœæ­¢
            if re.match(r"^(##\s|###\s|\*\*æ¶‰åŠæ–‡ä»¶|ğŸ“|âš¡|âœ…|### å†³ç­–)", line):
                break
            result_lines.append(line)

    # å¦‚æœæ²¡æœ‰æ•è·åˆ°çŸ¥è¯†ç‚¹åŒºå—ï¼Œå°è¯•æå–ä¸»é¢˜å’Œç®€è¦å†…å®¹
    if not result_lines:
        for line in lines:
            if "**ä¸»é¢˜**" in line or line.startswith("## ä¸»é¢˜"):
                result_lines.append(line)
            elif line.startswith("- **") and len(result_lines) < 8:
                result_lines.append(line)

    return "\n".join(result_lines).strip()


def load_existing_facts(project: str) -> dict:
    """è¯»å–é¡¹ç›®ç°æœ‰çš„ facts.json"""
    facts_path = AREAS_DIR / project / "facts.json"
    if facts_path.is_file():
        try:
            return json.loads(facts_path.read_text(encoding="utf-8"))
        except json.JSONDecodeError:
            return {"facts": []}
    return {"facts": []}


def get_next_fact_id(existing_facts: list) -> int:
    """ä»ç°æœ‰ facts ä¸­è·å–æœ€å¤§ ID ç¼–å·"""
    max_id = 0
    for fact in existing_facts:
        fid = fact.get("id", "")
        match = re.match(r"fact-(\d+)", fid)
        if match:
            max_id = max(max_id, int(match.group(1)))
    return max_id + 1


def build_prompt(project: str, knowledge_texts: list[str], existing_facts: list) -> tuple[str, str]:
    """ç»„è£… LLM prompt, è¿”å› (system_prompt, user_prompt)"""
    existing_fact_texts = []
    for f in existing_facts:
        if f.get("status") == "active":
            existing_fact_texts.append(f"- [{f.get('category', '')}] {f.get('fact', '')}")

    existing_section = ""
    if existing_fact_texts:
        existing_section = (
            "\n\nå·²æœ‰äº‹å® (ä¸è¦é‡å¤):\n" + "\n".join(existing_fact_texts)
        )

    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æå–åŠ©æ‰‹ã€‚ä»ç»™å®šçš„ä¼šè¯çŸ¥è¯†ç‚¹ä¸­æå– 1-3 æ¡æœ€æœ‰ä»·å€¼çš„åŸå­äº‹å®ã€‚"
        "\n\nè¦æ±‚:"
        "\n1. è¾“å‡ºçº¯ JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« fact (string, 50å­—ä»¥å†…) å’Œ category (string)"
        f"\n2. category å¿…é¡»æ˜¯ä»¥ä¸‹ä¹‹ä¸€: {', '.join(VALID_CATEGORIES)}"
        "\n3. åªæå–å¯¹æœªæ¥å¼€å‘æœ‰å‚è€ƒä»·å€¼çš„äº‹å®ï¼ˆæ¶æ„å†³ç­–ã€Bug æ¨¡å¼ã€å…³é”®é…ç½®ç­‰ï¼‰"
        "\n4. ä¸è¦é‡å¤å·²æœ‰äº‹å®"
        "\n5. ä¸è¦è¾“å‡ºä»»ä½• JSON ä»¥å¤–çš„å†…å®¹"
    )

    knowledge_combined = "\n\n---\n\n".join(knowledge_texts)

    user_prompt = (
        f"é¡¹ç›®: {project}\n"
        f"æœ¬å‘¨ä¼šè¯çŸ¥è¯†ç‚¹ ({len(knowledge_texts)} ä¸ªä¼šè¯):\n\n"
        f"{knowledge_combined}"
        f"{existing_section}"
    )

    return system_prompt, user_prompt


def call_haiku(system_prompt: str, user_prompt: str) -> str | None:
    """è°ƒç”¨ claude CLI (haiku model), è¿”å›åŸå§‹è¾“å‡º"""
    # å°† system prompt èå…¥ user promptï¼Œå› ä¸º claude CLI ä¼šåŠ è½½å…¨å±€æŒ‡ä»¤
    combined_prompt = (
        f"[æŒ‡ä»¤] {system_prompt}\n\n"
        f"[è¾“å…¥]\n{user_prompt}\n\n"
        f"[è¾“å‡º] åªè¾“å‡ºçº¯ JSON æ•°ç»„ï¼Œä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹:"
    )
    try:
        result = subprocess.run(
            [
                "claude",
                "-p",
                "--model", "haiku",
                "--no-session-persistence",
            ],
            input=combined_prompt,
            capture_output=True,
            text=True,
            timeout=LLM_TIMEOUT,
        )
        if result.returncode != 0:
            print(f"  [é”™è¯¯] claude è°ƒç”¨å¤±è´¥: {result.stderr.strip()}", file=sys.stderr)
            return None
        return result.stdout.strip()
    except FileNotFoundError:
        print("  [é”™è¯¯] claude å‘½ä»¤æœªæ‰¾åˆ°", file=sys.stderr)
        return None
    except subprocess.TimeoutExpired:
        print(f"  [è¶…æ—¶] claude è°ƒç”¨è¶…è¿‡ {LLM_TIMEOUT}s", file=sys.stderr)
        return None


def parse_response(raw: str) -> list[dict] | None:
    """
    è§£æ LLM å“åº”ä¸º fact åˆ—è¡¨ã€‚
    å¤šå±‚ fallback: json.loads â†’ æ­£åˆ™æå– [...] â†’ None
    """
    if not raw:
        return None

    # å°è¯•ç›´æ¥è§£æ
    try:
        data = json.loads(raw)
        if isinstance(data, list):
            return data
    except json.JSONDecodeError:
        pass

    # æ­£åˆ™æå– JSON æ•°ç»„
    match = re.search(r"\[.*\]", raw, re.DOTALL)
    if match:
        try:
            data = json.loads(match.group(0))
            if isinstance(data, list):
                return data
        except json.JSONDecodeError:
            pass

    print(f"  [è­¦å‘Š] æ— æ³•è§£æ LLM å“åº”: {raw[:200]}", file=sys.stderr)
    return None


def validate_facts(facts: list[dict]) -> list[dict]:
    """éªŒè¯å’Œæ¸…æ´— facts"""
    valid = []
    for f in facts:
        if not isinstance(f, dict):
            continue
        fact_text = f.get("fact", "").strip()
        category = f.get("category", "").strip()

        if not fact_text:
            continue
        if category not in VALID_CATEGORIES:
            category = "implementation"  # fallback

        valid.append({
            "fact": fact_text[:120],  # æˆªæ–­è¿‡é•¿
            "category": category,
        })

    return valid[:3]  # æœ€å¤š 3 æ¡


def write_facts(project: str, new_facts: list[dict], existing_data: dict, dry_run: bool) -> int:
    """è¿½åŠ æ–° facts åˆ° facts.json, è¿”å›å†™å…¥æ•°é‡"""
    if not new_facts:
        return 0

    project_dir = AREAS_DIR / project
    facts_path = project_dir / "facts.json"

    existing_facts = existing_data.get("facts", [])
    next_id = get_next_fact_id(existing_facts)
    today = datetime.now().strftime("%Y-%m-%d")

    for i, nf in enumerate(new_facts):
        fact_entry = {
            "id": f"fact-{next_id + i:03d}",
            "fact": nf["fact"],
            "evidence": "auto-extract-facts.py ä»æœ¬å‘¨ session æ‘˜è¦æå–",
            "timestamp": today,
            "status": "active",
            "category": nf["category"],
        }
        existing_facts.append(fact_entry)

    existing_data["facts"] = existing_facts

    if dry_run:
        print(f"  [é¢„è§ˆ] å°†å†™å…¥ {len(new_facts)} æ¡äº‹å®åˆ° {facts_path}")
        for nf in new_facts:
            print(f"    - [{nf['category']}] {nf['fact']}")
        return len(new_facts)

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    project_dir.mkdir(parents=True, exist_ok=True)

    facts_path.write_text(
        json.dumps(existing_data, ensure_ascii=False, indent=2) + "\n",
        encoding="utf-8",
    )
    print(f"  [å†™å…¥] {len(new_facts)} æ¡äº‹å® â†’ {facts_path}")
    return len(new_facts)


def process_project(
    project: str,
    sessions: list[Path],
    dry_run: bool,
) -> int:
    """å¤„ç†å•ä¸ªé¡¹ç›®, è¿”å›å†™å…¥äº‹å®æ•°"""
    print(f"\n--- {project} ({len(sessions)} ä¸ªä¼šè¯) ---")

    # æå–çŸ¥è¯†ç‚¹
    knowledge_texts = []
    for sp in sessions:
        kp = extract_knowledge_points(sp)
        if kp:
            knowledge_texts.append(kp)

    if not knowledge_texts:
        print("  [è·³è¿‡] æ— å¯ç”¨çŸ¥è¯†ç‚¹")
        return 0

    # åŠ è½½ç°æœ‰ facts
    existing_data = load_existing_facts(project)
    existing_facts = existing_data.get("facts", [])

    print(f"  çŸ¥è¯†ç‚¹: {len(knowledge_texts)} æ®µ, å·²æœ‰äº‹å®: {len(existing_facts)} æ¡")

    # æ„å»º prompt
    system_prompt, user_prompt = build_prompt(project, knowledge_texts, existing_facts)

    if dry_run:
        print(f"  [é¢„è§ˆ] prompt é•¿åº¦: system={len(system_prompt)}, user={len(user_prompt)}")
        # dry-run æ—¶ä¸è°ƒç”¨ LLMï¼Œæ˜¾ç¤ºå°†è¦å‘é€çš„ä¿¡æ¯
        print(f"  [é¢„è§ˆ] å°†è°ƒç”¨ claude haiku æå–äº‹å®")
        print(f"  [é¢„è§ˆ] æ¶‰åŠçŸ¥è¯†ç‚¹æ¥æº:")
        for sp in sessions:
            print(f"    - {sp.name}")
        return 0

    # è°ƒç”¨ LLM
    raw = call_haiku(system_prompt, user_prompt)
    if not raw:
        return 0

    # è§£æå“åº”
    parsed = parse_response(raw)
    if not parsed:
        return 0

    # éªŒè¯
    valid_facts = validate_facts(parsed)
    if not valid_facts:
        print("  [è·³è¿‡] æå–çš„äº‹å®å‡æ— æ•ˆ")
        return 0

    # å†™å…¥
    return write_facts(project, valid_facts, existing_data, dry_run=False)


def main():
    parser = argparse.ArgumentParser(
        description="ä» session æ‘˜è¦è‡ªåŠ¨æå–åŸå­äº‹å®"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="é¢„è§ˆæ¨¡å¼ï¼Œä¸è°ƒç”¨ LLM ä¹Ÿä¸å†™å…¥æ–‡ä»¶",
    )
    parser.add_argument(
        "--project",
        type=str,
        default=None,
        help="åªå¤„ç†æŒ‡å®šé¡¹ç›® (areas/projects ä¸‹çš„ç›®å½•å)",
    )
    parser.add_argument(
        "--days",
        type=int,
        default=7,
        help="æ‰«ææœ€è¿‘ N å¤©çš„ sessions (é»˜è®¤ 7)",
    )
    args = parser.parse_args()

    print(f"è‡ªåŠ¨äº‹å®æå– - {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    print(f"æ‰«æèŒƒå›´: æœ€è¿‘ {args.days} å¤©")
    if args.dry_run:
        print("[é¢„è§ˆæ¨¡å¼]")

    # æ‰«æ sessions
    grouped = scan_sessions(days=args.days)

    if not grouped:
        print("\næœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„ session æ‘˜è¦")
        return

    print(f"\nå‘ç° {len(grouped)} ä¸ªé¡¹ç›®:")
    for proj, sessions in sorted(grouped.items()):
        print(f"  - {proj}: {len(sessions)} ä¸ªä¼šè¯")

    # è¿‡æ»¤: å•é¡¹ç›®æ¨¡å¼æˆ– >= 2 ä¸ª session
    total_written = 0

    for proj in sorted(grouped.keys()):
        sessions = grouped[proj]

        if args.project and proj != args.project:
            continue

        if not args.project and len(sessions) < 2:
            print(f"\n--- {proj} ({len(sessions)} ä¸ªä¼šè¯) ---")
            print("  [è·³è¿‡] ä¼šè¯æ•° < 2")
            continue

        count = process_project(proj, sessions, dry_run=args.dry_run)
        total_written += count

    print(f"\nå®Œæˆ: å…±å†™å…¥ {total_written} æ¡æ–°äº‹å®")


if __name__ == "__main__":
    main()
